{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e456987-037a-441e-a347-a5776b37b4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Obter os parâmetros de entrada do job\n",
    "catalog_name = dbutils.widgets.get(\"catalog\")\n",
    "schema_name = dbutils.widgets.get(\"schema\")\n",
    "folder_volume = dbutils.widgets.get(\"folder_volume\") # Ex: api_meltano_20250617_2230\n",
    "\n",
    "# 2. Configurar os caminhos e o Spark\n",
    "volume_name = \"raw\"\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "# Caminho base onde todos os arquivos JSON da execução foram salvos\n",
    "source_path_base = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/{folder_volume}\"\n",
    "print(f\"Iniciando processamento de JSON para a pasta: {source_path_base}\")\n",
    "\n",
    "# 3. Listar todos os arquivos .jsonl que foram enviados nesta execução\n",
    "try:\n",
    "    # Vamos procurar por arquivos que terminam com .jsonl\n",
    "    json_files = [f.path for f in dbutils.fs.ls(source_path_base) if f.path.endswith('.jsonl')]\n",
    "    if not json_files:\n",
    "        print(f\"Aviso: Nenhum arquivo .jsonl encontrado em '{source_path_base}'.\")\n",
    "        dbutils.notebook.exit(\"Nenhum arquivo para processar.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao listar arquivos em '{source_path_base}'. Verifique se a pasta e os arquivos foram enviados. Erro: {e}\")\n",
    "    dbutils.notebook.exit(\"Falha ao encontrar o diretório de origem.\")\n",
    "\n",
    "# 4. Loop para processar cada arquivo JSON\n",
    "for file_path in json_files:\n",
    "    \n",
    "    # 5. Extrair o nome do arquivo para criar o nome da tabela\n",
    "    # Ex: '.../purchaseorderdetail.jsonl' -> 'purchaseorderdetail'\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    table_base_name = file_name.split('.')[0]\n",
    "    \n",
    "    # Constrói o nome final da tabela Delta: 'raw_api_purchaseorderdetail'\n",
    "    table_name_final = f\"raw_api_{table_base_name.lower()}\"\n",
    "    \n",
    "    print(f\"--- Processando Arquivo ---\")\n",
    "    print(f\"Origem JSONL: {file_path}\")\n",
    "    print(f\"Destino Delta: {table_name_final}\")\n",
    "\n",
    "    # 6. Ingestão dos dados usando COPY INTO (ideal para JSON)\n",
    "    try:\n",
    "        # COPY INTO é excelente para JSON, pois pode inferir o schema e lidar com evoluções.\n",
    "        # 'force' = 'true' permite que ele sobrescreva uma tabela se necessário, útil para reprocessamento.\n",
    "        # 'mergeSchema' = 'true' permite adicionar novas colunas se o schema do JSON mudar.\n",
    "        spark.sql(f\"\"\"\n",
    "            COPY INTO {catalog_name}.{schema_name}.{table_name_final}\n",
    "            FROM (SELECT * FROM json.`{file_path}`)\n",
    "            FILEFORMAT = JSON\n",
    "            COPY_OPTIONS ('force' = 'true', 'mergeSchema' = 'true')\n",
    "        \"\"\")\n",
    "        print(f\"Sucesso ao usar COPY INTO para a tabela {table_name_final}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback para criar a tabela com Spark Read/Write se COPY INTO falhar (ex: erro de sintaxe)\n",
    "        print(f\"COPY INTO falhou, tentando criar com Read/Write. Erro: {e}\")\n",
    "        try:\n",
    "            # Opção 'primitivesAsString'='true' ajuda a evitar erros de schema misto lendo tudo como texto inicialmente.\n",
    "            # 'multiLine'='true' é necessário se cada arquivo contiver um único objeto JSON formatado em várias linhas.\n",
    "            # Se for JSONL (um objeto por linha), você pode remover a opção multiLine.\n",
    "            df = spark.read.option(\"primitivesAsString\", \"true\").option(\"multiLine\", \"true\").json(file_path)\n",
    "            \n",
    "            # A função a seguir \"achata\" (flattens) o schema se ele for aninhado.\n",
    "            # Isso transforma {\"user\": {\"id\": 1}} em colunas user_id = 1.\n",
    "            def flatten_df(nested_df):\n",
    "                from pyspark.sql.functions import col\n",
    "                from pyspark.sql.types import StructType\n",
    "\n",
    "                flat_cols = [c[0] for c in nested_df.dtypes if c[1][:6] != 'struct']\n",
    "                nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n",
    "\n",
    "                flat_df = nested_df.select(flat_cols +\n",
    "                                           [col(nc + '.' + c).alias(nc + '_' + c)\n",
    "                                            for nc in nested_cols\n",
    "                                            for c in nested_df.select(nc + '.*').columns])\n",
    "                return flat_df\n",
    "\n",
    "            df_flat = flatten_df(df)\n",
    "\n",
    "            df_flat.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name_final}\")\n",
    "            print(f\"Sucesso ao criar a tabela {table_name_final} com Read/Write e achatamento de schema.\")\n",
    "        except Exception as e_write:\n",
    "            print(f\"ERRO: Falha crítica ao processar o arquivo {file_path} para a tabela {table_name_final}. Erro: {e_write}\")\n",
    "            continue # Pula para o próximo arquivo em caso de erro\n",
    "\n",
    "print(\"--- Processamento de todos os arquivos JSON concluído. ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6555369186299034,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "jsonl_to_delta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
