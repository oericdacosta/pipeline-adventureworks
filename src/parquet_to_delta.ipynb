{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7724d8d-58a3-422a-890d-e5eea5a46d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze_productionproduct\n"
     ]
    }
   ],
   "source": [
    "# 1. Obter os parâmetros de entrada do job\n",
    "catalog_name = dbutils.widgets.get(\"catalog\")\n",
    "schema_name = dbutils.widgets.get(\"schema\")\n",
    "folder_volume = dbutils.widgets.get(\"folder_volume\") # Ex: 20250617_1930\n",
    "\n",
    "# 2. Configurar os caminhos e o Spark\n",
    "volume_name = \"raw\"\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "# Caminho base onde todos os arquivos da execução foram salvos\n",
    "source_path_base = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/{folder_volume}\"\n",
    "print(f\"Iniciando processamento para a pasta: {source_path_base}\")\n",
    "\n",
    "# 3. Listar todos os diretórios de tabelas que foram enviados nesta execução\n",
    "# O Meltano cria um subdiretório para cada tabela (ex: ./output/Production-Product/)\n",
    "try:\n",
    "    table_directories = dbutils.fs.ls(source_path_base)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao listar diretórios em '{source_path_base}'. Verifique se a pasta e os arquivos foram enviados corretamente. Erro: {e}\")\n",
    "    dbutils.notebook.exit(\"Falha ao encontrar o diretório de origem.\")\n",
    "\n",
    "# 4. Loop para processar cada tabela\n",
    "for table_dir_info in table_directories:\n",
    "    # O nome do diretório é o nome da stream do Meltano (ex: 'Production-Product')\n",
    "    table_stream_name = table_dir_info.name.strip('/')\n",
    "    parquet_source_path = table_dir_info.path\n",
    "    \n",
    "    # 5. Construir o nome da tabela Delta final\n",
    "    # Converte 'Production-Product' para 'raw_meltano_production_product'\n",
    "    table_name_final = f\"raw_meltano_{table_stream_name.lower().replace('-', '_')}\"\n",
    "    \n",
    "    print(f\"--- Processando Tabela ---\")\n",
    "    print(f\"Origem Parquet: {parquet_source_path}\")\n",
    "    print(f\"Destino Delta: {table_name_final}\")\n",
    "\n",
    "    # 6. Ingestão dos dados usando COPY INTO ou Read/Write\n",
    "    try:\n",
    "        # COPY INTO é mais eficiente e idempotente para ingestão incremental\n",
    "        spark.sql(f\"\"\"\n",
    "            COPY INTO {catalog_name}.{schema_name}.{table_name_final}\n",
    "            FROM '{parquet_source_path}'\n",
    "            FILEFORMAT = PARQUET\n",
    "            COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "        \"\"\")\n",
    "        print(f\"Sucesso ao usar COPY INTO para a tabela {table_name_final}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback para criar a tabela se ela não existir\n",
    "        print(f\"COPY INTO falhou (talvez a tabela não exista ainda), tentando criar com Read/Write. Erro: {e}\")\n",
    "        try:\n",
    "            df = spark.read.format(\"parquet\").load(parquet_source_path)\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name_final}\")\n",
    "            print(f\"Sucesso ao criar a tabela {table_name_final} com Read/Write.\")\n",
    "        except Exception as e_write:\n",
    "            print(f\"ERRO: Falha ao processar os dados para a tabela {table_name_final}. Erro: {e_write}\")\n",
    "            # Continue para a próxima tabela em caso de falha em uma específica\n",
    "            continue\n",
    "\n",
    "print(\"--- Processamento de todas as tabelas concluído. ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bkp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
